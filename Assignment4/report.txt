Notes
- People image files removed from VOC data set using code
- Ground truth added for img2

Questions
4.a.i: 
Q: Explain what is linear about the linear Support Vector Machine.
A:
The hyperplane is linear. It is a linear function of the inputs.
A non-linear SVM can contain higher order terms, resulting in a hypersurface which may not be a straight 'linear' hyperplane.



4.b 
Q: Calculate the training (the data that was seen by the lSVM) and testing (the data that was not seen by the lSVM) performance by implementing the lines of code in Detector.cpp@349 to 373. Calculate conf_train and conf_val without making use of model.svm->predict(...)!
A: Done, validation code moved to other function (because val_data is not even available at the original position in the code!)



4.c 
Q: What, in terms of the theory of SVM, does the model consist of? Why does that look like a face?
A:
The model is a representation of the hyperplane, which by nature is the border between the positive and negative class i.e. the border between a face and not a face. It slightly resembles a face because it defines the furthest a face can be from looking like a face to still be classified as a face.



4.d.i 
Q: What does this C parameter do? 
A:
The c-parameter specifies the penalty of training vectors being on the wrong side of the decision surface, versus optimizing the margin between the closest vectors on the right side of the decision surface. Such a situation is inevitable when there exists no linear border between the vectors of each class.



4.d.ii 
Q: Find the optimal value for C, when using all the training images (or as many as fit in your computer's memory if you run into memory problems!). 
A:
For testing we used the following settings:
<images amount="1000" factor="10" width="20">
<features equalize="1" whiten="0">



Note: Because in the provided config file the C parameter was set to 1000, we started looking for the optimal C parameter in that order of magnitude, and found that all values of C lead to the same performance. Thinking it was an implementation mistake by your hand we kept the parameter set at 1000 throughout the tests. It was only after we had performed all tests that we again looked into the C parameter and realized that for large datasets the value should be well below 1.



4.d.iii 
Q:
    What do these switches [equalize and whiten] do with the learning data?
    Why is this good for the performance?
A:
They normalize the image pixels.
Equalization transforms all pixels at a given location over all images such that it has a mean value of zero and a standard deviation of 1.
Whitening removes the covariances between pixels so that neighboring pixels won't 'share information'.
This nulls the effect of vignetting for example; darker image corners will not be taken into account when trying to classify an image as a face or not.





5.b 
Q: Another problem aside from scaling is rotation. If you can think of a good way to do that, it can give you some nice bonus points. 
A:
Per layer of the pyramid, we could rotate the entire image around multiple times by a certain interval of degrees by which 360 is divisible, such as 36. The rotation needed to match one of these images would be up to 10 times smaller than the rotation needed to match the original image, which could hopefully be overcome by the model by itself if the interval is small enough. 



5.c 
Q: Generate detection results (images with detection boxes) for all test images not using the pyramid and using the pyramid. That's 2 result images per test image should be handed in. Name the files clearly (eg.: result_pixelmodel_nopyramid_img1.jpg), or put them in your report with a clear caption. 
A:




6.e
Q: Report the impact on the performance of the validation data (ie.: Give the change in percentage correct on the validation data.)
A:
	(amount="500" factor="10" equalize="1" whiten="1")
	SVM Validation using pixel model: 98.4% 
	SVM Validation using HOG model: 99.8%
	With HOG: +1.4% (or: 1,6/0,2 = 8x better)
	
	

6f
Q: Report the impact on the performance of the test images.
A: i: See 5c (with pyramid, with HOG)
ii: 

HOG, with pyramid
=======
6.f 
Q: Report the impact on the performance of the test images.
:
6.f.i
Q: Generate detection results (images with detection boxes) for all test images using the pyramid
A:
see 5.c

6.f.ii
Q: Report the Average Precision (see point 7) for the test image that came with ground truth data annotated in config.xml. Give the AP-score for the pixel model without the feature pyramid, the pixel model with the feature pyramid and the HOG model with the feature pyramid. Also hand in the optimal settings used to train each of these models.
A:
see data\compare_PR_normal_pyramid_hog\[x]
where [x] = 
pr_img1_hogmodel_pyramid.pdf
pr_img1_pixelmodel_nopyramid.pdf
pr_img1_pixelmodel_pyramid.pdf
pr_img2_hogmodel_pyramid.pdf
pr_img2_pixelmodel_nopyramid.pdf
pr_img2_pixelmodel_pyramid.pdf

The caption of the plots give the area under the curve.

HOG features seem to have a negative effect on the detection performance. Perhaps this is due to the fact that the exact location of the detected face cannot relyable be reconstructed, since multiple pixels are contained in each cell.
Also using a pyramid seems to have a negative effect on the detection performance. This might be due to the fact that all faces in the images were of similar size.

7.b
Q: Put the Precision/Recall curves of your models for img1.jpg in your report (or add (an) image(s) of the graphs to your archive).
A:



7.c
Q: Calculate the AP-scores of the images as described in point 6f.ii.
A:
see 6.f.ii
