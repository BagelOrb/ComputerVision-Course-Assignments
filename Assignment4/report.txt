Notes
- People image files removed from VOC data set using code
- Ground truth added for img2

Questions
4.a.i: 
Q: Explain what is linear about the linear Support Vector Machine.
A:
The hyperplane is linear. It is a linear function of the inputs.
A non-linear SVM can contain higher order terms, resulting in a hypersurface which may not be a straight 'linear' hyperplane.



4.b 
Q: Calculate the training (the data that was seen by the lSVM) and testing (the data that was not seen by the lSVM) performance by implementing the lines of code in Detector.cpp@349 to 373. Calculate conf_train and conf_val without making use of model.svm->predict(...)!
A: Done, validation code moved to other function (because val_data is not even available at the original position in the code!)



4.c 
Q: What, in terms of the theory of SVM, does the model consist of? Why does that look like a face?
A:
The model is a representation of the hyperplane, which by nature is the border between the positive and negative class i.e. the border between a face and not a face. It slightly resembles a face because it defines the furthest a face can be from looking like a face to still be classified as a face.



4.d.i 
Q: What does this C parameter do? 
A:
The c-parameter specifies the penalty of training vectors being on the wrong side of the decision surface, versus optimizing the margin between the closest vectors on the right side of the decision surface. Such a situation is inevitable when there exists no linear border between the vectors of each class.



4.d.ii 
Q: Find the optimal value for C, when using all the training images (or as many as fit in your computer's memory if you run into memory problems!). 
A:
TODO (implement validation first)



4.d.iii 
Q:
    What do these switches [equalize and whiten] do with the learning data?
    Why is this good for the performance?
A:
They normalize the image pixels.
Equalization transforms all pixels at a given location over all images such that it has a mean value of zero and a standard deviation of 1.
Whitening removes the covariances between pixels so that neighboring pixels won't 'share information'.
This nulls the effect of vignetting for example; darker image corners will not be taken into account when trying to classify an image as a face or not.





5.b 
Q: Another problem aside from scaling is rotation. If you can think of a good way to do that, it can give you some nice bonus points. 
A:
Per layer of the pyramid, we could rotate the entire image around multiple times by a certain interval of degrees by which 360 is divisible, such as 36. The rotation needed to match one of these images would be up to 10 times smaller than the rotation needed to match the original image, which could hopefully be overcome by the model by itself if the interval is small enough. 



5.c 
Q: Generate detection results (images with detection boxes) for all test images not using the pyramid and using the pyramid. That's 2 result images per test image should be handed in. Name the files clearly (eg.: result_pixelmodel_nopyramid_img1.jpg), or put them in your report with a clear caption. 
A:




6.e
Q: Report the impact on the performance of the validation data (ie.: Give the change in percentage correct on the validation data.)
A:
	(amount="500" factor="10" equalize="1" whiten="1")
	SVM Validation using pixel model: 98.4% 
	SVM Validation using HOG model: 99.8%
	With HOG: +1.4% (or: 1,6/0,2 = 8x better)
	
	

6f
Q: Report the impact on the performance of the test images.
A: i: See 5c (with pyramid, with HOG)
ii: 
img1:
Pixel, no pyramid:
50,0,0
49,1,0.142857
48,1,0.142857
47,1,0.142857
46,1,0.142857
45,1,0.142857
44,1,0.142857
43,1,0.142857
42,1,0.142857
41,1,0.214286
40,1,0.214286
39,1,0.285714
38,1,0.285714
37,1,0.285714
36,1,0.285714
35,1,0.357143
34,1,0.357143
33,1,0.357143
32,1,0.357143
31,0.833333,0.357143
30,0.714286,0.357143
29,0.625,0.357143
28,0.454545,0.357143
27,0.5,0.428571
26,0.5,0.428571
25,0.5,0.5
24,0.5,0.5
23,0.388889,0.5
22,0.368421,0.5
21,0.363636,0.571429
20,0.307692,0.571429
19,0.266667,0.571429
18,0.25,0.571429
17,0.222222,0.571429
16,0.2,0.571429
15,0.204545,0.642857
14,0.18,0.642857
13,0.169811,0.642857
12,0.155172,0.642857
11,0.155172,0.642857
10,0.147541,0.642857
9,0.15625,0.714286
8,0.140845,0.714286
7,0.135135,0.714286
6,0.125,0.714286
5,0.120482,0.714286
4,0.111111,0.714286
3,0.10101,0.714286
2,0.0961538,0.714286
1,0.0961538,0.714286
0,0.0917431,0.714286

Pixel, with pyramid
50,0,0
49,1,0.142857
48,1,0.142857
47,1,0.142857
46,1,0.142857
45,1,0.142857
44,1,0.142857
43,0.666667,0.142857
42,0.666667,0.142857
41,0.75,0.214286
40,0.75,0.214286
39,0.8,0.285714
38,0.8,0.285714
37,0.8,0.285714
36,0.8,0.285714
35,0.714286,0.357143
34,0.714286,0.357143
33,0.714286,0.357143
32,0.555556,0.357143
31,0.454545,0.357143
30,0.384615,0.357143
29,0.3125,0.357143
28,0.3125,0.357143
27,0.3,0.428571
26,0.272727,0.428571
25,0.241379,0.5
24,0.2,0.5
23,0.189189,0.5
22,0.166667,0.5
21,0.148936,0.5
20,0.153846,0.571429
19,0.137931,0.571429
18,0.125,0.571429
17,0.119403,0.571429
16,0.109589,0.571429
15,0.101266,0.571429
14,0.0963855,0.571429
13,0.0952381,0.571429
12,0.0941176,0.571429
11,0.0941176,0.571429
10,0.1,0.642857
9,0.0978261,0.642857
8,0.0909091,0.642857
7,0.0891089,0.642857
6,0.0865385,0.642857
5,0.0849057,0.642857
4,0.0803571,0.642857
3,0.0782609,0.642857
2,0.0756303,0.642857
1,0.0743802,0.642857
0,0.0737705,0.642857

HOG, with pyramid
18,0,0
17,0.0277778,0.0714286
16,0.025641,0.0714286
15,0.0232558,0.0714286
14,0.0217391,0.0714286
13,0.0196078,0.0714286
12,0.0172414,0.0714286
11,0.0169492,0.0714286


img2:
Pixel, no pyramid:
50,0,0
49,1,0.0769231
48,1,0.153846
47,1,0.153846
46,1,0.153846
45,1,0.153846
44,1,0.153846
43,1,0.153846
42,1,0.153846
41,1,0.153846
40,1,0.153846
39,1,0.153846
38,1,0.153846
37,1,0.153846
36,1,0.153846
35,1,0.230769
34,0.75,0.230769
33,0.428571,0.230769
32,0.3,0.230769
31,0.285714,0.307692
30,0.208333,0.384615
29,0.185185,0.384615
28,0.138889,0.384615
27,0.116279,0.384615
26,0.106383,0.384615
25,0.0925926,0.384615
24,0.0882353,0.461538
23,0.0731707,0.461538
22,0.0736842,0.538462
21,0.0630631,0.538462
20,0.0555556,0.538462
19,0.0567376,0.615385
18,0.0509554,0.615385
17,0.0467836,0.615385
16,0.042328,0.615385
15,0.040201,0.615385
14,0.0377358,0.615385
13,0.0396476,0.692308
12,0.0379747,0.692308
11,0.0409836,0.769231
10,0.0387597,0.769231
9,0.037594,0.769231
8,0.03663,0.769231
7,0.0358423,0.769231
6,0.0350877,0.769231
5,0.034965,0.769231
4,0.0341297,0.769231
3,0.03367,0.769231
2,0.0334448,0.769231
1,0.0330033,0.769231
0,0.0322581,0.769231
Pixel, with pyramid

HOG, with pyramid
=======
6.f 
Report the impact on the performance of the test images.
i
Q: Generate detection results (images with detection boxes) for all test images using the pyramid
A:
see 5.c

ii
Q: Report the Average Precision (see point 7) for the test image that came with ground truth data annotated in config.xml. Give the AP-score for the pixel model without the feature pyramid, the pixel model with the feature pyramid and the HOG model with the feature pyramid. Also hand in the optimal settings used to train each of these models.
A:



7.b
Q: Put the Precision/Recall curves of your models for img1.jpg in your report (or add (an) image(s) of the graphs to your archive).
A:



7.c
Q: Calculate the AP-scores of the images as described in point 6f.ii.
A:

